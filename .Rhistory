defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(0)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
###The Big Short
##The Big Short: Interest Rates Explained
set.seed(seed = NULL)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
n <- 1000
loss_per_foreclosure <- -200000
p <- 0.02
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
B <- 10000
losses <- replicate(B, {
defaults <- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE)
sum(defaults * loss_per_foreclosure)
})
losses
n*(p*loss_per_foreclosure + (1-p)*0)
sqrt(n)*abs(loss_per_foreclosure)*sqrt(p*(1-p))
#We can now set an interest rate to guarantee that, on average, we break even.
#Basically, we need to add a quantity  x to each loan, which in this case are represented by draws,
#so that the expected value is 0. If we define  l to be the loss per foreclosure, we need:
#l*p + x(1-p) = 0
#which implies x is
loss_per_foreclosure*p/(1-p)
#We can now set an interest rate to guarantee that, on average, we break even.
#Basically, we need to add a quantity  x to each loan, which in this case are represented by draws,
#so that the expected value is 0. If we define  l to be the loss per foreclosure, we need:
#l*p + x(1-p) = 0
#which implies x is
x <- loss_per_foreclosure*p/(1-p)
interest_rate <- loss_per_foreclosure * p + x(1-p)
x <- loss_per_foreclosure*p/(1-p)
interest_rate <- loss_per_foreclosure * p + x(1-p)
#We can now set an interest rate to guarantee that, on average, we break even.
#Basically, we need to add a quantity  x to each loan, which in this case are represented by draws,
#so that the expected value is 0. If we define  l to be the loss per foreclosure, we need:
#l*p + x(1-p) = 0
#which implies x is
interest_rate <- loss_per_foreclosure*p/(1-p)
loss_per_foreclosure * p + x(1-p)
loss_per_foreclosure * p + interest_rate * (1-p)
l <- loss_per_foreclosure
z <- qnorm(0.01)
x <- -l*( n*p - z*sqrt(n*p*(1-p)))/ ( n*(1-p) + z*sqrt(n*p*(1-p)))
x
profit_per_loan <- loss_per_foreclosure*p + x*(1-p)
profit_per_loan
total_expected_profit <- n*(loss_per_foreclosure*p + x*(1-p))
total_expected_profit
B <- 100000
profit <- replicate(B, {
draws <- sample( c(x, loss_per_foreclosure), n,
prob=c(1-p, p), replace = TRUE)
sum(draws)
})
mean(profit)
mean(profit<0)
library(dslabs)
data(tissue_gene_expression)
dim(tissue_gene_expression$x)
table(tissue_gene_expression$y)
dist(tissue_gene_expression$x, distance='maximum')
dist(tissue_gene_expression)
dist(tissue_gene_expression$x)
cor(tissue_gene_expression$x)
d <- dist(tissue_gene_expression$x)
as.matrix(d)[c(1,2,39,40,73,74), c(1,2,39,40,73,74)]
image(d)
image(as.matrix(d))
library(tidyverse)
library(dslabs)
data("mnist_27")
mnist_27$test%>% ggplot(aes(x_1, x_2, color = y)) +  geom_point()
library(caret)
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family="binomial")
p_hat_logistic <- predict(fit_glm, mnist_27$test)
y_hat_logistic <- factor(ifelse(p_hat_logistic > 0.5, 7, 2))
confusionMatrix(data = y_hat_logistic, reference = mnist_27$test$y)$overall[1]
?knn3
knn_fit <- knn3(y ~ ., data = mnist_27$train)
##KNN
set.seed(1)
library(caret)
library(dslabs)
library(purrr)
library(tidyverse)
data(heights)
y <- heights$sex
x <- heights$height
set.seed(1)
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- heights[-train_index,]
train_set <- heights[train_index,]
ks <- seq(1, 101, 3)
F_1 <- map_df(ks, function(k){
set.seed(1)
fit <- knn3(sex ~ height, data = train_set, k = k)
y_hat <- predict(fit, test_set, type = "class")
F_val <- F_meas(data = y_hat, reference = factor(test_set$sex))
list(k=k, F_val=F_val)
})
F_1
F_1 %>% na.omit() %>% ggplot(aes(k, F_val)) + geom_line()
max(F_1$F_val)
best_k <- F_1$k[which.max(F_1$F_val)]
best_k
max(F_1)
F_1 %>% slice(which.max(F_1$F_val))
library(caret)
library(dslabs)
library(purrr)
library(tidyverse)
data(heights)
y <- heights$sex
x <- heights$height
set.seed(1)
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- heights[-train_index,]
train_set <- heights[train_index,]
ks <- seq(1, 101, 3)
F_1 <- map_df(ks, function(k){
set.seed(1)
fit <- knn3(sex ~ height, data = train_set, k = k)
y_hat <- predict(fit, test_set, type = "class")
F_val <- F_meas(data = y_hat, reference = factor(test_set$sex))
list(k=k, F_val=F_val)
})
F_1
F_1 %>% na.omit() %>% ggplot(aes(k, F_val)) + geom_line()
max(F_1$F_val)
best_k <- F_1$k[which.max(F_1$F_val)]
best_k
F_1 %>% slice(which.max(F_1$F_val))
##KNN
library(caret)
library(dslabs)
library(purrr)
library(tidyverse)
data(heights)
y <- heights$sex
x <- heights$height
set.seed(1)
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- heights[-train_index,]
train_set <- heights[train_index,]
ks <- seq(1, 101, 3)
F_1 <- sapply(ks, function(k){
set.seed(1)
fit <- knn3(sex ~ height, data = train_set, k = k)
y_hat <- predict(fit, test_set, type = "class")
F_val <- F_meas(data = y_hat, reference = factor(test_set$sex))
list(k=k, F_val=F_val)
})
F_1
max(F_1$F_val)
max(F_1)
ks <- seq(1, 101, 3)
F_1 <- sapply(ks, function(k){
set.seed(1)
fit <- knn3(sex ~ height, data = train_set, k = k)
y_hat <- predict(fit, test_set, type = "class")
F_val <- F_meas(data = y_hat, reference = factor(test_set$sex))
#list(k=k, F_val=F_val)
})
F_1
max(F_1)
library(caret)
library(dslabs)
library(purrr)
library(tidyverse)
data(heights)
y <- heights$sex
x <- heights$height
set.seed(1)
f1 <- sapply(ks, function(k) {
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights[test_index, ]
test_set <- heights[-test_index, ]
fit <- knn3(sex ~ height, data = train_set, k = k)
y_hat <- predict(fit, test_set, type = "class") %>% factor(levels = levels(test_set$sex))
F_meas(data = y_hat, reference = factor(train_set$sex))
})
max(f1)
k <- seq(1, 101, 3)
rm(list = ls())
source('~/.active-rstudio-document', echo=TRUE)
rm(list = ls())
library(dslabs)
library(caret)
data("tissue_gene_expression")
set.seed(1)
split_data <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
split_data <- createDataPartition(tissue_gene_expression, times = 1, p = 0.5, list = FALSE)
k = seq(1, 11, 2)
library(dslabs)
library(caret)
data("tissue_gene_expression")
set.seed(1)
split_data <- createDataPartition(tissue_gene_expression, times = 1, p = 0.5, list = FALSE)
k = seq(1, 11, 2)
head(split_data)
split_data <- createDataPartition(tissue_gene_expression$y, times = 1, p = 0.5, list = FALSE)
head(split_data)
train <- tissue_gene_expression[split_data,]
test <- tissue_gene_expression[-split_data,]
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
split_data <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
head(split_data)
test <- tissue_gene_expression[-split_data,]
train <- tissue_gene_expression[split_data,]
x <- tissue_gene_expression$x
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
train_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set_x <- x[-train_index,]
train_set_x <- x[train_index,]
test_set_y <- y[-train_index]
train_set_y <- y[train_index]
ks = seq(1, 11, 2)
ks = seq(1, 11, 2)
accuracy <- sapply(ks, function(k){
fit <- knn3(train_set_x, train_set_y, k = k)
y_hat <- predict(fit, test_set_x, type = "class")
match <- confusionMatrix(data = y_hat, reference = test_set_y)$overall["Accuracy"]
list(k=k, match=match)
})
accuracy
accuracy <- map_df(ks, function(k){
fit <- knn3(train_set_x, train_set_y, k = k)
y_hat <- predict(fit, test_set_x, type = "class")
match <- confusionMatrix(data = y_hat, reference = test_set_y)$overall["Accuracy"]
list(k=k, match=match)
})
accuracy
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
set.seed(1)
fit <- train(x, y, method = "knn", tuneGrid = data.frame(k = seq(1,11,2)))
fit
rm(list = ls())
source('~/.active-rstudio-document', echo=TRUE)
y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)
library(HistData)
set.seed(1983)
galton_heights <- GaltonFamilies %>%
filter(gender == "male") %>%
group_by(family) %>%
sample_n(1) %>%
ungroup() %>%
select(father, childHeight) %>%
rename(son = childHeight)
#Suppose you are tasked with building a machine learning algorithm that predicts
#the son’s height  Y  using the father’s height  X. Let’s generate testing and training sets:
y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)
#In this case, if we were just ignoring the father’s height and guessing the son’s height,
#we would guess the average height of sons.
m <- mean(train_set$son)
m
mean((m - test_set$son)^2)
fit <- lm(son ~ father, data = train_set)
fit$coef
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)
##The predict function
#The predict function takes a fitted object from functions such as lm or glm
#and a data frame with the new predictors for which to predict.
y_hat <- predict(fit, test_set)
y_hat
y_hat <- predict(fit, test_set)
mean((y_hat - test_set$son)^2)
rain_set %>%
filter(round(height)==66) %>%
summarize(y_hat = mean(sex=="Female"))
train_set %>%
filter(round(height)==66) %>%
summarize(y_hat = mean(sex=="Female"))
force(mnist_27)
heights %>%
mutate(x = round(height)) %>%
group_by(x) %>%
filter(n() >= 10) %>%
summarize(prop = mean(sex == "Female")) %>%
ggplot(aes(x, prop)) +
geom_point()
`summarise()` ungrouping output (override with `.groups` argument)
#Since the results from the plot above look close to linear, we will try regression. We assume that:
# p(x) = Pr(Y = 1 | X = x) = β0 + β1x
# Note: beacuse p0(x) = 1 - pa(x), we wo;; only estimate p1(x) and drop the 1 index.
#If we convert the factors to 0s and 1s, we can estimate β0 and β1 with least squares.
lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>% lm(y ~ height, data = .)
lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>% lm(y ~ height, data = .)
p_hat <- predict(lm_fit, test_set)
y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)[["Accuracy"]]
heights %>%
mutate(x = round(height)) %>%
group_by(x) %>%
filter(n() >= 10) %>%
summarize(prop = mean(sex == "Female")) %>%
ggplot(aes(x, prop)) +
geom_point() +
geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
range(p_hat)
glm_fit <- train_set %>%
mutate(y = as.numeric(sex == "Female")) %>%
glm(y ~ height, data=., family = "binomial")
#We can obtain prediction using the predict function:
p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")
y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male") %>% factor
confusionMatrix(y_hat_logit, test_set$sex)[["Accuracy"]]
#We can obtain prediction using the predict function:
p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family = "binomial")
p_hat_glm <- predict(fit_glm, mnist_27$test, type="response")
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 7, 2))
confusionMatrix(y_hat_glm, mnist_27$test$y)$overall["Accuracy"]
p_hat <- predict(fit_glm, newdata = mnist_27$true_p, type = "response")
mnist_27$true_p %>% mutate(p_hat = p_hat) %>%
ggplot(aes(x_1, x_2,  z=p_hat, fill=p_hat)) +
geom_raster() +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
stat_contour(breaks=c(0.5), color="black")
library(dslabs)
data("heights")
y <- heights$height
set.seed(2) #if you are using R 3.5 or earlier
train_set %>%
filter(round(height)==66) %>%
#As an example, let’s provide a prediction for a student that is 66 inches tall.
#What is the conditional probability of being female if you are 66 inches tall?
#In our dataset, we can estimate this by rounding to the nearest inch and computing:
train_set %>%
filter(round(height)==66) %>%
summarize(y_hat = mean(sex=="Female"))
train_set %>%
filter(round(height)==66) %>%
summarize(y_hat = mean(sex=="Female"))
library(dslabs)
data("heights")
y <- heights$height
set.seed(2) #if you are using R 3.5 or earlier
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)
#The regression approach can be extended to categorical data. In this section we
#first illustrate how, for binary data, one can simply assign numeric values of 0 and 1
#to the outcomes y, and apply regression as if the data were continuous.
#We will then point out a limitation with this approach and introduce logistic regression as a solution.
#Logistic regression is a specific case of a set of generalized linear models.
#To illustrate logistic regression, we will apply it to our previous predicting sex example:
#If we define the outcome Y as 1 for females and 0 for males, and X as the height,
#we are interested in the conditional probability:
#  Pr(Y = 1 ∣ X = x)
#As an example, let’s provide a prediction for a student that is 66 inches tall.
#What is the conditional probability of being female if you are 66 inches tall?
#In our dataset, we can estimate this by rounding to the nearest inch and computing:
train_set %>%
filter(round(height)==66) %>%
summarize(y_hat = mean(sex=="Female"))
heights %>%
mutate(x = round(height)) %>%
group_by(x) %>%
filter(n() >= 10) %>%
summarize(prop = mean(sex == "Female")) %>%
ggplot(aes(x, prop)) +
geom_point()
#Since the results from the plot above look close to linear, we will try regression. We assume that:
# p(x) = Pr(Y = 1 | X = x) = β0 + β1x (Conditional Probability of Y = 1 given x is a line intercept + slope 8 height)
# Note: beacuse p0(x) = 1 - pa(x), we wo;; only estimate p1(x) and drop the 1 index.
#If we convert the factors to 0s and 1s, we can estimate β0 and β1 with least squares.
lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>% lm(y ~ height, data = .)
lm_fit
p_hat <- predict(lm_fit, test_set)
y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)[["Accuracy"]]
heights %>%
mutate(x = round(height)) %>%
group_by(x) %>%
filter(n() >= 10) %>%
summarize(prop = mean(sex == "Female")) %>%
ggplot(aes(x, prop)) +
geom_point() +
geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
#The range is
range(p_hat) # -0.331 1.036
glm_fit <- train_set %>%
mutate(y = as.numeric(sex == "Female")) %>%
glm(y ~ height, data=., family = "binomial")
glm_fit
#We can obtain prediction using the predict function:
p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")
y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male") %>% factor
confusionMatrix(y_hat_logit, test_set$sex)[["Accuracy"]]
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family = "binomial")
p_hat_glm <- predict(fit_glm, mnist_27$test, type="response")
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 7, 2))
confusionMatrix(y_hat_glm, mnist_27$test$y)$overall["Accuracy"]
library(tidyverse)
library(dslabs)
data("polls_2008")
qplot(day, margin, data = polls_2008)
`geom_smooth()` using formula 'y ~ x'
span <- 7
fit <- with(polls_2008,
ksmooth(day, margin, kernel = "box", bandwidth = span))
polls_2008 %>% mutate(smooth = fit$y) %>%
ggplot(aes(day, margin)) +
geom_point(size = 3, alpha = .5, color = "grey") +
geom_line(aes(day, smooth), color="red")
span <- 7
fit <- with(polls_2008,
ksmooth(day, margin, kernel = "normal", bandwidth = span))
polls_2008 %>% mutate(smooth = fit$y) %>%
ggplot(aes(day, margin)) +
geom_point(size = 3, alpha = .5, color = "grey") +
geom_line(aes(day, smooth), color="red")
total_days <- diff(range(polls_2008$day))
span <- 21/total_days
fit <- loess(margin ~ day, degree=1, span = span, data=polls_2008)
polls_2008 %>% mutate(smooth = fit$fitted) %>%
ggplot(aes(day, margin)) +
geom_point(size = 3, alpha = .5, color = "grey") +
geom_line(aes(day, smooth), color="red")
